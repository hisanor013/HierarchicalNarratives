{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3009ed8c",
   "metadata": {},
   "source": [
    "# Module and function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e15a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdfplumber\n",
    "import tiktoken \n",
    "import openai\n",
    "import tqdm\n",
    "import random\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "\n",
    "def extract_text_with_pdfplumber(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def split_text_by_delimiter(text, delimiter):\n",
    "    return text.split(delimiter)\n",
    "\n",
    "def split_into_sentences(text_0):\n",
    "    # 不要な改行を取り除く\n",
    "    \n",
    "    text = copy.deepcopy(text_0)\n",
    "    text = re.sub(r'\\s*\\n\\s*', '', text.strip())\n",
    "    \n",
    "    # 「。」で文を分割し、それぞれの文の後に「。」を追加する\n",
    "    sentences = [sentence.strip() + '。' for sentence in text.split('。') if sentence]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def split_list_into_groups(sentences, group_size=6):\n",
    "    # Split the list into groups of approximately `group_size` elements each\n",
    "    grouped_sentences = [sentences[i:i + group_size] for i in range(0, len(sentences), group_size)]\n",
    "    return grouped_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8449a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f03bc667",
   "metadata": {},
   "source": [
    "# PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d2d6dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdfs = [\"log_public/94024601_01.pdf\",\"log_public/94041201_01.pdf\",\"log_public/94059201_01.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd0fbd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#opinions = []\n",
    "#delimiter = \"●受付番号\"\n",
    "#for pdf in pdfs:\n",
    "#    #print(pdf)\n",
    "#    # Extract text using pdfplumber\n",
    "#    extracted_text = extract_text_with_pdfplumber(pdf)\n",
    "\n",
    "#    # Split the text by your specific delimiter\n",
    "#    sections = split_text_by_delimiter(extracted_text, delimiter)\n",
    "\n",
    "#    # Print the sections\n",
    "#    for index, section in enumerate(sections):\n",
    "#        opinions.append(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894bec8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82a15b91",
   "metadata": {},
   "source": [
    "# Clean texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6ba19fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the identifier from each string in the list\n",
    "#clean_opinions_0 = [text.split('\\n', 1)[-1] if '\\n' in text else text for text in opinions[1:]]\n",
    "\n",
    "## Remove unnecessary short text\n",
    "#clean_opinions = []\n",
    "#for i in range(len(clean_opinions_0)):\n",
    "#    if len(clean_opinions_0[i]) > 2:\n",
    "#        clean_opinions.append(clean_opinions_0[i])\n",
    "        \n",
    "#print(\"Opinion in total: \" + str(len(clean_opinions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819d8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ec73b45",
   "metadata": {},
   "source": [
    "# Load dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e744b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"log_public/gen_ai_opinions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb36e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "549a3614",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab3f2c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "use_model = \"gpt-4-turbo-2024-04-09\"\n",
    "openai.api_key = \"YOUR-KEY\"\n",
    "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "model = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "START_FROM_SCRATCH = 1\n",
    "if START_FROM_SCRATCH != 1:\n",
    "\n",
    "    log_list = glob.glob(\"log_public/public_separated_*\" )\n",
    "    print(log_list)\n",
    "\n",
    "    record2hantei = dict()\n",
    "    file_list = log_list \n",
    "    for file in file_list:\n",
    "        print(file)\n",
    "        load_record(file,record2hantei)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23830ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96dc488a",
   "metadata": {},
   "source": [
    "# Create new log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6cd3a971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_public/public_separated_C_api_gpt-4-turbo-2024-04-09_2024-10-24T22_33_00.log\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "LOG_FILE = os.path.join(\"log_public\", \"public_separated_C_api_\" +  use_model  + \"_\"+ now.strftime('%Y-%m-%dT%H_%M_%S') + \".log\") \n",
    "\n",
    "if os.path.exists(\"log_public\") == False:\n",
    "    os.makedirs(\"log_public\")\n",
    "\n",
    "print(LOG_FILE)\n",
    "\n",
    "def get_completion(messages, model=use_model):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_format={\"type\":\"json_object\"},\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "DOLLAR_PER_TOKEN = 0.002/1000\n",
    "YEN_PER_DOLLAR= 139.69\n",
    "\n",
    "# トークン数とコストを計算\n",
    "def check_tokens(prompt, model):\n",
    "    tokens = model.encode(prompt)\n",
    "    num_tokens = len(tokens)\n",
    "    cost_dollar = len(tokens)*DOLLAR_PER_TOKEN\n",
    "    cost_yen = len(tokens)*DOLLAR_PER_TOKEN*YEN_PER_DOLLAR\n",
    "#     print(\"num_tokens = {}: cost = ${}, ¥{}\".format(num_tokens, cost_dollar, cost_yen))\n",
    "    return num_tokens, cost_dollar, cost_yen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86f2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d8e321f",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cbff9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_1 = \"\"\"\n",
    "次の文章には生成AIの規制に関する意見が含まれています。以下の手順に従って解析してください。\n",
    "\n",
    "「この」、「あの」、「その」など文章内の指示語が用いられている部分が何を指しているのかを明確にして指示語の置き換えリストを出力してください。エッジリストを作成する際にはこの指示語の置き換えを参照し、それらの指示語を具体的な内容に置き換えるように注意してください。\n",
    "\n",
    "さらに以下のパターンに基づいて情報をエッジリスト形式で抽出してください。ほとんど全ての文がこのパターンのいずれかに分類されます。そのためできるだけ全ての文を分類するようにしてください。また、できるだけ元の表現をそのまま使うようにし、勝手に文章に書いてある表現を変化させないようにしてください。Pattern1とPattern2とPattern4の「誰が」の部分が意見を書いた人と思われる時は「筆者」としてください。\n",
    "\n",
    "Pattern1: 「誰が, 何に対して, どうあるべきである」という規範的主張\n",
    "Pattern2: 「誰が, 誰に対して, どうして欲しい」という要望\n",
    "Pattern3: 「何が・誰が、何に対して、どういう状況を引き起こしているか」という因果関係\n",
    "Pattern4: 「誰が, 何に対して, どう捉えているか」という認識\n",
    "\n",
    "エッジリストの形式は以下の通りです：\n",
    "Pattern1 (誰が, 何に対して, どうあるべきである)\n",
    "Pattern2 (誰が, 誰に対して, どうして欲しい)\n",
    "Pattern3 (何が・誰が, 何に対して, どういう状況を引き起こしているか)\n",
    "Pattern4 (誰が, 何に対して, どう捉えているか)\n",
    "Pattern4 (誰が,(何が・誰が,何に対して,どういう状況を引き起こしているか))\n",
    "\n",
    "注意: Pattern4はPattern3を内包する場合があります。例えば：Pattern4 (誰が, (何が・誰が,何に対して,どういう状況を引き起こしているか))なのでPattern3を内包するように書ける時はそっちを優先してください。\n",
    "\n",
    "全ての文に対してPattern1からPattern4のいずれかに該当するか確認してください。指示語の置き換えリストに含まれている言い換え表現を、エッジリストで必ず使用するようにしてください。また、出力結果のエッジリストには元の文章に存在しないフレーズが含まれていないことを確認し、元の文章にないフレーズの出現回数が0であることを保証してください。\n",
    "\n",
    "出力例は以下の通りです。必ずこの形式を守ってください。説明は一切出力せず、このJSONフォーマットの回答以外を出力しないでください。\n",
    "出力例：\n",
    "{\"指示語の置き換え\":[\"「この文言」 = 「憲法999に関する文言」\"],\n",
    "\"エッジリスト\":[\"Pattern1 (筆者, 水星人対策, 喫緊の課題である)\",\"Pattern2 (筆者, 政府, 水星人追放を求める)\",\"Pattern3 (水星人, 金星人, やる気を無くさせている)\",\"Pattern4 (筆者, 水星人の核兵器, この世の終わりである)\"]}\n",
    "=====\n",
    "%s\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a1372db",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_2 = \"\"\"\n",
    "次に元の文章とこれまで抽出したPattern1からPattern4までの結果を元に以下の手順に従って解析してください：\n",
    "\n",
    "まず「この」、「あの」、「その」など文章内の指示語が用いられている部分が何を指しているのかを明確にして指示語の置き換えリストを出力してください。エッジリストを作成する際にはこの指示語の置き換えを参照し、それらの指示語を具体的な内容に置き換えるように注意してください。\n",
    "\n",
    "さらに以下のパターンに基づいて情報をエッジリスト形式で抽出してください。できるだけ元の表現をそのまま使うようにし、勝手に文章に書いてある表現を変化させないようにしてください。ここで新たにPattern1からPattern4そのものを抽出する必要はなく、今回はPattern5だけに絞ってください。\n",
    "\n",
    "Pattern5: 因果関係(Pattern3)や認識 (Pattern4) が他の要素 (Pattern1の規範的主張、Pattern2の要望) に影響を与えている場合の関連性\n",
    "\n",
    "エッジリストの形式は以下の通りです：\n",
    "Pattern5 (誰が,(何が・誰が,何に対して,どういう状況を引き起こしているか),(何に対して,どうあるべきである))\n",
    "Pattern5 (誰が,(何に対して,どう捉えているか), (何に対して,どうあるべきである))\n",
    "Pattern5 (誰が,(何が・誰が,何に対して,どういう状況を引き起こしているか),(誰に対して,どうして欲しい))\n",
    "Pattern5 (誰が, (何に対して,どう捉えているか), (誰に対して,どうして欲しい))\n",
    "\n",
    "全てのPattern1とPattern2について、以前に抽出したPattern1からPattern4までとの対応関係を検証し、Pattern5のどの条件に該当するか確認してください。テキスト内で「誰が」の部分が意見を書いた人と思われる時は「筆者」としてください。指示語の置き換えリストに含まれている言い換え表現を、エッジリストで必ず使用するようにしてください。また、出力結果のエッジリストには元の文章に存在しないフレーズが含まれていないことを確認し、元の文章にないフレーズの出現回数が0であることを保証してください。さらに、指定されたエッジリストの形式を厳守し、それよりも多くの要素を持つタプルなどを出力しないでください。\n",
    "\n",
    "Pattern1の規範的主張やPattern2の要望が存在しない場合は、エッジリストを空で返してください。抽出されたPatternは、原文を正確に保ち、一言一句変更せずに出力してください。最も重要なのは、元の文章を無断で変更しないことです。\n",
    "\n",
    "出力例は以下の通りです。必ずこの形式を守ってください。説明は一切出力せず、このJSONフォーマットの回答以外を出力しないでください。\n",
    "\n",
    "出力例：\n",
    "{\"指示語の置き換え\":[\"「この文言」 = 「憲法999に関する文言」\"],\n",
    "\"エッジリスト\":[\"Pattern5 (筆者, (水星人の襲来,物価の高騰), (消費税,増税を見送るべきである))\",\"Pattern5 (筆者, (水星人対策,必要である), (政府,水星人追放を求める))\"]}\n",
    "====\n",
    "====\n",
    "%s\"\"\"\n",
    "#jsonize_prompt = \"\"\"Please answer the output in JSON format. \n",
    "#example:\n",
    "#{\n",
    "#    \"YesOrNo\":\"...\"\n",
    "#}\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebdfd04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9028200",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2ea97ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "START_INDEX = 0\n",
    "print(START_INDEX)\n",
    "print(LOG_FILE)\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbffb328",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "for kkk in range(5):\n",
    "    print(\"TRY: \" + str(kkk) + \", Start \" + str(START_INDEX))\n",
    "    try:\n",
    "        #### FIRST HALF ####\n",
    "        #for i in tqdm.tqdm(range(START_INDEX,MID_INDEX)):\n",
    "        #### SECOND HALF OR ALL ####\n",
    "        for i in tqdm.tqdm(range(START_INDEX,len(df))):\n",
    "        #### DEBUG #####\n",
    "        #for i in tqdm.tqdm(range(START_INDEX,START_INDEX+3)):\n",
    "            #### \n",
    "            sentiment = df[\"sentiment\"].iloc[i]\n",
    "            t =  df[\"text\"].iloc[i]\n",
    "            \n",
    "            if t not in record2hantei:\n",
    "            \n",
    "                # Clean\n",
    "                sentences = split_into_sentences(t)\n",
    "                grouped_sentences = split_list_into_groups(sentences)\n",
    "\n",
    "                tmp_edges = []\n",
    "                for jjj in range(len(grouped_sentences)):    \n",
    "                    text = \"\".join(grouped_sentences[jjj])\n",
    "                    prompt = prompt_template_1 % text\n",
    "                    messages = [\n",
    "                        {\n",
    "                            \"role\" : \"user\",\n",
    "                            \"content\" : prompt\n",
    "                        }\n",
    "                    ]\n",
    "                    tmp_response_1 = get_completion(messages)\n",
    "                    num_tokens1, cost_dollar1, cost_yen1 = check_tokens(tmp_response_1, model)\n",
    "                    result = {\n",
    "                        \"index\": i,\n",
    "                        \"moto\": t,\n",
    "                        \"subindex\": kkk,\n",
    "                        \"type\": \"A\",\n",
    "                        \"input\": text,\n",
    "                        \"sentiment\": sentiment,\n",
    "                        \"cost\":{\n",
    "                                1:{\n",
    "                                \"num_tokens1\":num_tokens1,\n",
    "                                \"cost_dollar1\":cost_dollar1,\n",
    "                                \"cost_yen1\":cost_yen1\n",
    "                            }\n",
    "                        },\n",
    "                        \"answer\":tmp_response_1\n",
    "                    }\n",
    "                    tmp_edges.extend(json.loads(tmp_response_1)['エッジリスト'])\n",
    "\n",
    "                    #### KOKODE HAKIDASU \n",
    "                    with open(LOG_FILE, \"a\") as f:\n",
    "                        print(json.dumps(result), file=f)\n",
    "\n",
    "                        \n",
    "     \n",
    "                proceed_hantei = 0\n",
    "                for ppp in range(len(tmp_edges)):\n",
    "                    if \"Pattern1\" in tmp_edges[ppp]:\n",
    "                        proceed_hantei = 1\n",
    "                        break\n",
    "                        \n",
    "                    if \"Pattern2\" in tmp_edges[ppp]:\n",
    "                        proceed_hantei = 1\n",
    "                        break\n",
    "          \n",
    "                if proceed_hantei == 1:           \n",
    "                        \n",
    "                    text_2 =  \"\\n\".join(tmp_edges) + \"\\n\"  + \"\".join(sentences)\n",
    "                    record = str(sentiment) + \";\" + text_2\n",
    "                    prompt = prompt_template_2 % text_2\n",
    "                    messages = [\n",
    "                        {\n",
    "                            \"role\" : \"user\",\n",
    "                            \"content\" : prompt\n",
    "                        }\n",
    "                    ]\n",
    "                    tmp_response_2 = get_completion(messages)\n",
    "                    #num_tokens1, cost_dollar1, cost_yen1 = check_tokens(tmp_response_2, model)\n",
    "                    result_2 = {\n",
    "                        \"index\": i,\n",
    "                        \"moto\": t,\n",
    "                        \"type\": \"B\",\n",
    "                        \"input\": text_2,\n",
    "                        \"sentiment\": sentiment,\n",
    "                        \"cost\":{\n",
    "                            1:{\n",
    "                                \"num_tokens1\":num_tokens1,\n",
    "                                \"cost_dollar1\":cost_dollar1,\n",
    "                                \"cost_yen1\":cost_yen1\n",
    "                            }\n",
    "                        },\n",
    "                        \"answer\":tmp_response_2\n",
    "                    }\n",
    "                    #### KOKODE HAKIDASU \n",
    "                    with open(LOG_FILE, \"a\") as f:\n",
    "                        print(json.dumps(result_2), file=f)\n",
    "\n",
    "            START_INDEX = i\n",
    "                \n",
    "    except:\n",
    "        START_INDEX = i\n",
    "        time.sleep(5.4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e68f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfad8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b24cd73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
